<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Fundamentals of Data Analytics and Predictions</title>

    <meta name="author" content="Xi (Rossi) LUO">

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="reveal.js/dist/theme/white.css" id="theme">


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="reveal.js/plugin/highlight/tomorrow-night-blue.css">
    <!--
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
-->
    <link rel="stylesheet" href="./css/rossisimple.css" id="theme">
    <!--    <link rel="stylesheet" href="./css/brightRoom.css" id="theme">-->
</head>

<body>

    <div class="reveal">
        <credit></credit>

        <!-- Any section element inside of this container is displayed as a slide -->
        <div class="slides">


            <section>
                <h2><strong>
                        Fundamentals of
                        <highlight>Data Analytics</highlight> and
                        <emph>Predictions</emph>
                    </strong></h2>
                <br>
                <h2>Regularization</h2>
                <br>

                <h3>Xi (Rossi) <strong>LUO</strong></h3>
                <br>
                <p><small>Department of Biostatistics and Data Science<br>
                        School of Public Health<br>The University of Texas Health Science Center at Houston</small></p>
            </section>


            <section>
                <h2>Outline</h2>
                <ul>
                    <li>Linear Predictors</li>
                    <li>Ridge Regression</li>
                    <li>Lasso</li>
                    <li>Elastic Net</li>
                </ul>
            </section>


            <section>
                <h2>Linear Predictors</h2>
                <ul>
                    <li>Input: $X=x_1, \dots, x_p$</li>
                    <li>Output: $Y$</li>
                    <li>The linear regression model is $Y=\beta_0+\sum_{j=1}^{p}X_j\beta_j+\epsilon$</li>
                    <li>To find least squares solution, want to minimize $E(y-f(x))^2$</li>
                    <li>Approximated by the empirical loss $RSS(\beta)=\sum_{i=1}^{i=N}(y_i-f(x_i))^2$</li>
                    <li>Has closed-form solution $\hat \beta=(X'X)^{-1}X'y$</li>
                </ul>
            </section>


            <section>
                <h2>Linear Model With Many Predictors —1</h2>
                <ul>
                    <li>Even though we have a linear model that seems to fit the data well, we may still have some
                        problems.</li>
                    <ul>
                        <li>If we have many predictors, the prediction accuracy may not be optimal.</li>
                        <li>If we have many predictors, interpretation may be difficult</li>
                    </ul>
                </ul>
            </section>


            <section>
                <h2>Linear Model With Many Predictors —2</h2>
                <ul>
                    <li>Two possible solutions:</li>
                    <ul>
                        <li>Use a subset of the predictors</li>
                        <li>Shrink some of the coefficients toward zero</li>
                    </ul>
                </ul>
            </section>


            <section>
                <h2>Subset (Variable) Selection</h2>
                <ul>
                    <li>Consider a case where the true model is $Y=\beta_0+\beta_1X_1+\beta_2X_2+\epsilon$ and $X_1$ and
                        $X_2$ are almost perfectly correlated (co-linear)</li>
                    <li>If we remove $X_2$ from the model, we can approximate the model by:
                        $Y=\beta_0+(\beta_1+\beta_2)X_1+\epsilon$ and get a good estimate of $Y$ estimating 2 parameters
                        instead of 3</li>
                    <li>Estimate will be a bit biased but we may lower the variance considerably.</li>
                </ul>
            </section>


            <section>
                <h2>Simulated Example</h2>
                <ul>
                    <li>Consider fitting various subset models to a simulated example where the true model is a linear
                        model with 10 predictors and we observe 15 outcomes</li>
                    <li>Select a number of covariates to include in the model, then look at all possible combinations of
                        covariates and pick one with the smallest RSS</li>
                    <li>Apply to a training and test set</li>
                </ul>
            </section>


            <section>
                <h2>Simulated Example</h2>
                <img src="img/lecture11_1.png" alt="" width="90%;">
            </section>


            <section>
                <h2>Prostate Data Example —1</h2>
                <ul>
                    <li>Outcome: prostate specific antigen (PSA)</li>
                    <li>Covariates:</li>
                    <ul>
                        <li>lcavol(log cancer volume)</li>
                        <li>lweight(log prostate weight)</li>
                        <li>Age</li>
                        <li>agelbph</li>
                        <li>log(benign prostatic hyperplasia amount)</li>
                        <li>svi(seminal vesicle invasion)</li>
                        <li>lcp(log capsular penetration)</li>
                        <li>gleasonscore</li>
                        <li>% gleasonscores 4 or 5.</li>
                    </ul>
                </ul>
            </section>


            <section>
                <h2>Prostate Data Example —2</h2>
                <img src="img/lecture11_2.png" alt="">
            </section>


            <section>
                <h2>Subset (Variable) Selection Summary —1</h2>
                <ul>
                    <li>Pros:</li>
                    <ul>
                        <li>Easy to interpret</li>
                        <li>Variable is either in or out</li>
                        <li>Improved prediction error over the full model</li>
                    </ul>
                    <li>Cons:</li>
                    <ul>
                        <li>Has high variance</li>
                        <li>How to choose the number of covariates to include?</li>
                        <li>The choice of one covariate over an another can sometimes be a very arbitrary decision
                            (especially with correlated predictors)</li>
                    </ul>
                </ul>
            </section>


            <section>
                <h2>Subset (Variable) Selection Summary —2</h2>
                <ul>
                    <li>We can think of the subset selection procedure as one that shrinks some of the coefficients to
                        0. What if we do this in a smoother way?</li>
                    <li>Instead, can shrink the regression coefficients in a continuous manner (instead of multiplying
                        each coefficient by 0 or 1, can multiply by a number between 0 and 1</li>
                    <li>Methods include ridge regression, the lasso, least angle regression, and elastic nets</li>
                </ul>
            </section>


            <section>
                <h1>Ridge Regression</h1>
            </section>


            <section>
                <h2>Ridge Regression —1</h2>
                <ul>
                    <li>Recall that the least squares fitting procedure estimates $\beta_0, \beta_1, \dots, \beta_p$
                        using the values that minimize
                        $RRS(\beta)=\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2$</li>
                    <li>In contrast, the ridge regression coefficient estimate $\hat\beta^R$ are the values that
                        minimize</li>
                    <h4>$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}\beta_j^2=RSS+\lambda\sum_{j=1}^{p}\beta_j^2$
                    </h4>
                    where $\lambda\ge0$ is a tuning parameter, to be determined separately
                </ul>
            </section>


            <section>
                <h2>Ridge Regression —2</h2>
                <ul>
                    <li>As with least squares, ridge regression seeks coefficient estimates that fit the data well, by
                        making the RSS small</li>
                    <li>However, the second term, $\lambda\sum_{j=1}^{p}\beta_j^2$ called a <strong>shrinkage
                            penalty</strong>, is small when $\beta_1, \dots, \beta_p$ are close to zero, and so it has
                        the effect of <strong>shrinking</strong> the estimates of $\beta_j$ towards zero</li>
                    <li>The tuning parameter $\lambda$ serves to control the relative impact of these two terms on the
                        regression coefficient estimates</li>
                    <li>Selecting a good value for $\lambda$ is critical, cross-validation is used for this</li>
                </ul>
            </section>


            <section>
                <h2>Credit Data Example</h2>
                <img src="img/lecture11_3.png" alt="">
            </section>


            <section>
                <h2>Scaling Predictors</h2>
                <ul style="font-size: 80%;">
                    <li>The standard least squares coefficient estimates are <strong>scale equivalent</strong>;
                        multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient
                        estimates by a factor of $\frac{1}{c}$. In other words, regardless of how the jthpredictor is
                        scaled, $X_j\hat\beta_j$ will remain the same</li>
                    <li>In contrast, the ridge regression coefficient estimates can change substantially when
                        multiplying a given predictor by a constant, due to the sum of squared coefficients term in the
                        penalty part of the ridge regression objective function</li>
                    <li>Therefore, it is best to apply ridge regression after <strong>standardizing the
                            predictors</strong>, using the formula</li>
                    $\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\bar{x}_j)^2}}$
                </ul>
            </section>


            <section>
                <h3>Why Ridge Regression Better Than Least Squares</h3>
                <img src="img/lecture11_4.png" alt="">
                <ul style="font-size: 75%;">
                    <li>
                        <h4>Simulated data with $n=50$ observations and $p=45$ predictors, all having nonzero
                            coefficients.</h4>
                    </li>
                    <li>
                        <h4>Squared bias (black), variance (green) and test mean square error (purple) for the ridge
                            regression on a simulated dataset as a function of $\lambda$ and $\frac{\lVert
                            \hat{\beta}_\lambda^R \rVert_2}{\lVert \hat{\beta} \rVert_2 }$</h4>
                    </li>
                    <li>
                        <h4>The horizontal dashed line indicates the minimum possible MSE</h4>
                    </li>
                    <li>
                        <h4>The purple crosses indicate the ridge regression models for which the MSE is smallest</h4>
                    </li>
                </ul>
            </section>


            <section>
                <h2>Comments on Ridge Regression</h2>
                <ul>
                    <li>History</li>
                    <ul>
                        <li>Andrey Tikhonov (Russian 1906-1993): Tikhonov regularization or Tikhonov-Phillips
                            regularization for ill-posed problem</li>
                        <li>Arthur E. Hoerl(1962, 1970): Ridge regression</li>
                    </ul>
                    <li>Note that this helps problem when have many correlated variables since in a linear regression
                        model, the coefficients can exhibit high variance</li>
                    <li>However, ridge regression includes ALL $p$ predictors (unlike subset selection)</li>
                </ul>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/aLnKyKfhlLvxz0f1cEi98?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/bgGHyRy8PIVc3KTAqTzp4?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <h1>Lasso</h1>
            </section>


            <section>
                <h2>The Lasso —1</h2>
                <ul style="font-size: 80%;">
                    <li>Ridge regression does have one obvious disadvantage: unlike subset selection, which will
                        generally select models that involve just a subset of the variables, ridge regression will
                        include all p predictors in the final model</li>
                    <li>The <strong>Lasso</strong> is a relatively recent alternative to ridge regression that overcomes
                        this disadvantage. The lasso coefficients, $\hat{\beta}_\lambda^L$ minimize the quantity</li>
                    <h5>$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda \sum_{j=1}^{p}\mid\beta_j\mid
                        =RSS+\lambda\sum_{j=1}^{p}\mid\beta_j\mid$</h5>
                    <li>In statistical parlance, the lasso uses an $ℓ_1$ (pronounced “ell 1”) penalty instead of an
                        $ℓ_2$ penalty. The $ℓ_1$ norm of a coefficient vector $\beta$ is given by $\lVert \beta \rVert
                        _1=\sum\mid\beta_j\mid$</li>
                </ul>
            </section>


            <section>
                <h2>The Lasso —2</h2>
                <ul>
                    <li>As with ridge regression, the lasso shrinks the coefficient estimates towards zero</li>
                    <ul>
                        <li>However, in the case of lasso, the $ℓ_1$ penalty has the effect of forcing some of the
                            coefficient estimates to be exactly zero when the tuning parameter $\lambda$ is sufficiently
                            large</li>
                        <li>Hence, much like best subset selection, the lasso performs <strong>variable
                                selection</strong>.</li>
                        <li>We say that lasso yields <strong>sparse</strong> models, that is, models that involve only a
                            subset of the variables</li>
                    </ul>
                    <li>As in ridge regression, selecting a good values of $\lambda$ for the lasso is critical;
                        cross-validation is again the method of choice</li>
                </ul>
            </section>


            <section>
                <h2>Example: Credit Dataset</h2>
                <img src="img/lecture11_5.png" alt="">
            </section>


            <section>
                <h2>The Variable Selection Property of the Lasso -1</h2>
                <ul>
                    <li>Why is it that the lasso, unlike ridge regression, results in coefficient estimates that are
                        exactly equal to zero?</li>
                </ul>
            </section>


            <section>
                <h2>The Variable Selection Property of the Lasso —2</h2>
                <ul style="font-size: 80%;">
                    <li>Why is it that the lasso, unlike ridge regression, results in coefficient estimates that are
                        exactly equal to zero?</li>
                    <li>One can show that the lasso and ridge regression coefficient estimates solve the problems</li>
                    Lasso: $$\underset{\beta}{\text{minimize}}\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2
                    \quad
                    \text{subject to}\sum_{j=1}^{p}\mid\beta_j\mid\le s$$
                    Ridge: $$\underset{\beta}{\text{minimize}}\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2
                    \quad
                    \text{subject to}\sum_{j=1}^{p}\beta_j^2\le s$$
                </ul>
            </section>

            <!-- <section data-state="ridgelasso">
                <h2>Graphical</h2>
                <img src="https://res.cloudinary.com/dyd911kmh/image/upload/v1648205672/image18_a3zz7y.png" alt="">
                <style>
                    .ridgelasso credit:after {
                        content: "Image source: https://res.cloudinary.com/dyd911kmh/image/upload/v1648205672/image18_a3zz7y.png";
                    }
                </style>
            </section> -->

            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/rxbV73SMgR7eaKk107kVW?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/DqaoTNGo2O1iyt0ryvR0d?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <h2>R Session</h2>
                <br>
                <div>
                    <a href="https://github.com/rluo/rluo.github.io/blob/master/statml/regularization.ipynb">
                        <i class="fa-solid fa-person-digging fa-beat-fade fa-2x"></i>
                        &nbsp; &nbsp;
                        <span class="rbtn">
                            code
                        </span>
                    </a>
                </div>
            </section>


            <section>
                <h1>Comparing Ridge and Lasso</h1>
            </section>


            <section>
                <h2>Comments on Ridge and Lasso: Ridge</h2>
                <ul>
                    <li>Ridge in $\mathbb{R} ^2$: $\lambda\sum\beta_j^2$, $\beta_1^2+\beta_2^2 \le s$</li>
                </ul>
                <img src="img/lecture11_6.png" alt="">
            </section>


            <section>
                <h2>Comments on Ridge and Lasso: Lasso</h2>
                <ul>
                    <li>Lasso in $\mathbb{R} ^2$: $\lambda\sum\mid\beta_j\mid$, $\mid\beta_1\mid+\mid\beta_2\mid \le s$
                    </li>
                </ul>
                <img src="img/lecture11_7.png" alt="">
            </section>


            <section>
                <h2>Comparing Lasso and Ridge Regression —1</h2>
                <img src="img/lecture11_8.png" alt="">
                <ul style="font-size: 70%;">
                    <li>Left plot: Plots of squared bias (black), variance (green), and test MSE (purple)for the lasso
                        on simulated data set</li>
                    <li>Right plot: Comparison of squared bias, variance and test MSE between lasso (solid) and ridge
                        (dashed). Both are plotted against their $R^2$ on the training data, as a form of indexing.</li>
                    <li>The crosses in both plots indicate the lasso model which the MSE is smallest</li>
                </ul>
            </section>


            <section>
                <h2>Comparing Lasso and Ridge Regression —2</h2>
                <img src="img/lecture11_9.png" alt="">
                <ul style="font-size: 70%;">
                    <li>Left plot: Plots of squared bias (black), variance (green), and test MSE (purple)for the lasso
                        on simulated data set. The simulated data is similar, except that now only two predictors are
                        related to response</li>
                    <li>Right plot: Comparison of squared bias, variance and test MSE between lasso (solid) and ridge
                        (dashed). Both are plotted against their $R^2$ on the training data</li>
                    <li>The crosses in both plots indicate the lasso model which the MSE is smallest</li>
                </ul>
            </section>


            <section>
                <h2>Conclusions</h2>
                <ul>
                    <li>These two examples illustrate that neither ridge regression nor the lasso will universally
                        dominate the other</li>
                    <li>In general, one might expect the lasso to perform better when the response is a function of only
                        a relatively small number of predictors</li>
                    <li>However, the number of predictors that is related to the response is never know a priori for
                        real data sets</li>
                    <li>A technique such as cross-validation can be used in order to determine which approach is better
                        on a particular data set</li>
                </ul>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/0cv2wXPR3oLtknveGQz0F?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/Vj6ZVhJINClKTf7PxvVsa?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/8x8yuJSpEksMrOSbmDkUl?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <h1>Select Tuning Parameters</h1>
            </section>


            <section>
                <h2>Selecting Tuning Parameters for Ridge Regression and Lasso</h2>
                <ul style="font-size: 80%;">
                    <li>As with subset selection, for ridge regression and lasso we require a method to determine which
                        of the models under consideration is best</li>
                    <li>We require a method selecting a value for the tuning parameter $\lambda$ or equivalently, the
                        value of the constraint $s$</li>
                    <li>Cross-validation provides a simple way to tackle this problem. We choose a grid of $\lambda$
                        values, and compute the cross-validation error rate for each value of $\lambda$</li>
                    <li>We then select the tuning parameter value for which the cross-validation error is smallest</li>
                    <li>Finally, the model is re-fit using all of the available observations and the selected value of
                        the tuning parameter</li>
                </ul>
            </section>


            <section>
                <h2>Credit Data Example</h2>
                <img src="img/lecture11_10.png" alt="">
                <ul style="font-size: 70%;">
                    <li>Left: Cross-validation error that result from applying ridge regression to the credit dataset
                        with various values of $\lambda$</li>
                    <li>Right: The coefficient estimates as a function of $\lambda$.</li>
                    <li>The vertical dashed lines indicate the value of $\lambda$ selected by cross validation</li>
                </ul>
            </section>


            <section>
                <h2>Simulated Data Example</h2>
                <img src="img/lecture11_11.png" alt="">
                <ul style="font-size: 70%;">
                    <li>Left: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated dataset</li>
                    <li>Right: The corresponding lasso coefficient estimates are displayed.</li>
                    <li>The vertical dashed line lines indicate the lasso fit for which the cross-validation error is
                        smallest</li>
                </ul>
            </section>


            <section>
                <h2>Generalize Lasso and Ridge</h2>
                <ul>
                    <li>We can generalize the constraints:</li>
                    <h4>$\hat\beta=\underset{\beta}{arg \,
                        min}\{\sum_{i=1}^{N}(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j)^2+\lambda\sum_{j=1}^{p}\mid\beta_j\mid^q\}$
                    </h4>
                    <ul>
                        <li>Note:</li>
                        <ul>
                            <li>if $q=1\Rightarrow$ lasso</li>
                            <li>And if $q=2\Rightarrow$ ridge regression</li>
                        </ul>
                    </ul>
                    <li>Can compromise with $1\lt q\lt 2$ but we won't get the lasso property of being able to set some
                        $\beta$'s to 0.</li>
                    <li>In R, can use the Lars function</li>
                </ul>
            </section>


            <section>
                <h2>Elastic Net</h2>
                $\lambda\sum_{j=1}^{p}(\alpha\beta_j^2+(1-\alpha)\mid\beta_j\mid)$
                <br>
                <ul>
                    <li>Weighted sum of lasso and ridge regression penalties</li>
                    <ul>
                        <li>Selects variable like the lasso</li>
                        <li>Shrinks together the coefficient of correlated predictors like ridge regression</li>
                    </ul>
                    <li>In R, <code>enet</code> function in <code>elasticnet</code> or <code>glmnet</code> from
                        <code>glmnet</code> library (note $\alpha$ may be scaled by different constants in packages)
                    </li>

                </ul>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/GUTrNtZIX5WQy3AYnSk2N?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <h1>Dimensionality Reduction and PCA</h1>
            </section>


            <section>
                <h2>Details of Dimension Reduction Methods —1</h2>
                <ul style="font-size: 80%;">
                    <li>Let $Z_1, Z_2, \dots, Z_M$ represent $M \lt p$ linear combinationsof our original $p$
                        predictors. That is $Z_m=\sum_{j=1}^{p}\phi_{mj}X_j$; for some constants $\phi_{m1}, \dots,
                        \phi_{mp}$ (1)</li>
                    <li>We can then fit the linear regression model using ordinary least squares model
                        $y_i=\theta_0+\sum_{m=1}^{M}\theta_mz_{im}+\epsilon_i, i=1, \dots, n$ (2)</li>
                    <li>Note in the second model, the regression coefficients are given by $\theta_0, \theta_1, \dots,
                        \theta_M$. If the constants $\phi_{m1}, \dots, \phi_{mp}$ are chosen wisely, then such dimension
                        reduction approaches can often outperform OLS regression</li>
                </ul>
            </section>


            <section>
                <h2>Details of Dimension Reduction Methods—2</h2>
                <ul style="font-size: 80%;">
                    <li>Note from equation 1 in slide 2</li>
                    $\sum_{m=1}^{M}\theta_mz_{im}=\sum_{m=1}^{M}\theta_m\sum_{j=1}^{p}\phi_{mj}x_{ij}$
                    <br>
                    $=\sum_{j=1}^{p}\sum_{m=1}^{M}\theta_m\phi_{mj}x_{ij}=\sum_{j=1}^{p}\beta_jx_{ij}$
                    <br>
                    where $\beta_j=\sum_{m=1}^{M}\theta_m\phi_{mj}$ (3)
                    <li>Hence model (2) can be thought of as a special case of the original linear regression
                        model</li>
                    <li>Dimension reduction serves to constrain the estimated $\beta_j$ coefficients, since now
                        they must take the form (3)</li>
                    <li>Can win in the bias-variance tradeoff</li>
                </ul>
            </section>


            <section>
                <h2>Principal Components Regression</h2>
                <ul>
                    <li>Here we apply principal components analysis (PCA) to define the linear combinations of the
                        predictors, for use in our regression</li>
                    <li>The first principal component is that (normalized) linear combination of the variables with the
                        largest variance</li>
                    <li>The second principal component has largest variance, subject to being uncorrelated with the
                        first. This continues on $\dots$</li>
                    <li>Hence with many correlated original variables, we replace them with a small set of principal
                        components that capture their joint variation</li>
                </ul>
            </section>


            <section>
                <h2>Pictures of PCA —1</h2>
                <img src="img/lecture11_12.png" alt="">
                <ul>
                    <li>The population size and ad spending for 100 different cities are shown as purple circles.</li>
                    <li>The green solid line indicates the first principal component and the blue dashed line indicates
                        the second principal component</li>
                </ul>
            </section>


            <section>
                <h2>Pictures of PCA —2</h2>
                <img src="img/lecture11_13.png" alt="">
                <ul>
                    <li>Left: The first principal component, chosen to minimize the sum of the squared perpendicular
                        distances to each point, is shown in green.</li>
                    <li>Right: The left-hand panel has been rotated so that the first principal component lies on the
                        x-axis</li>
                </ul>
            </section>


            <section>
                <h2>Pictures of PCA: $z_{i1}$ & $z_{i2}$</h2>
                <div class="inlinemiddle twocol" style="width: 45%;">
                    <img src="img/lecture11_14.png">
                </div>
                <div class="inlinemiddle twocol" style="width: 50%;">
                    <ul style="font-size: 80%;">
                        <li>Plots of the first principal component scores $z_{i1}$ versus population and ad spending
                        </li>
                        <li>The relationships are strong</li>
                        <br>
                        <br>
                        <li>Plots of the first principal component scores $z_{i2}$ versus population and ad spending
                        </li>
                        <li>The relationships are weak</li>
                    </ul>
                </div>
            </section>


            <section>
                <h2>Application to Principal Components Regression</h2>
                <img src="img/lecture11_15.png" alt="">
                <ul style="font-size: 80%;">
                    <li>PCR was applied to two simulated data sets. The black, green and purple lines correspond to
                        squared bias, variance, and test mean squared error, respectively</li>
                    <li>Left: All 45 predictors are relevant</li>
                    <li>Right: Only 2 predicators are relevant</li>
                </ul>
            </section>


            <section>
                <h2>Choosing the Number of Directions $M$</h2>
                <img src="img/lecture11_16.png" alt="">
                <ul style="font-size: 80%;">
                    <li>Left: PCR standardized coefficient estimates on the Credit data set for different values of $M$
                    </li>
                    <li>Right: The 10-fold cross validation MSE obtained using PCR, as a function of $M$</li>
                </ul>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/u7aZaEi79kCPIEdiQvpJz?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>


            <section>
                <h1>Partial Least Squares</h1>
            </section>


            <section>
                <h2>Partial Least Squares —1</h2>
                <ul>
                    <li>PCR identifies linear combinations, or directions, that best represent the predictors $X_1, X_2,
                        \dots, X_p$</li>
                    <li>These directions are identified in an <strong>unsupervised</strong> way, since the response Y is
                        not used to help determine the principal component directions</li>
                    <li>That is, the response does not <strong>supervise</strong> the identification of the principal
                        components</li>
                    <li>Consequently, PCR suffers from a potentially serious drawback: there is no guarantee that the
                        directions that best explain the predicators will also be the best directions to use for
                        predicting the response</li>
                </ul>
            </section>


            <section>
                <h2>Partial Least Squares —2</h2>
                <ul>
                    <li>Like PCR, <strong>Partial Least Squares</strong> (PLS) is a dimension reduction method, which
                        first identifies a new set of features $Z_1, Z_2, \dots, Z_M$ that are linear combinations of
                        the original features, and then fits a linear model via OLS using M new features</li>
                    <li>Unlike PCR, PLS identifies these new features in supervised way, that is, it makes use of the
                        response Yin order to identify new features that not only approximate the old features well, but
                        also that are related to the response</li>
                    <li>The PLS approach attempts to find directions that help explain both the response and the
                        predictors</li>
                </ul>
            </section>


            <section>
                <h2>Details of Partial Least Squares</h2>
                <ul>
                    <li>After standardizing the $p$ predictors, PLS computes the first $Z_1$ by setting each $\phi_{1j}$
                        in (Dimensionality Reduction and PCA equation 1) equal to the coefficient from the simple
                        linear regression of $Y$ onto $X_j$</li>
                    <li>One can show that this coefficient is proportional to the correlation between $Y$ and $X_j$</li>
                    <li>Hence in computing $Z_1=\sum_{j=1}^{p}\phi_{1j}X_j$, PLS places the highest weight on the
                        variables that are strongly related to the response</li>
                    <li>Subsequent directions are found by taking residuals and then repeating the above prescription
                    </li>
                </ul>
            </section>


            <section>
                <h2>Summary</h2>
                <ul>
                    <li>Model selection methods are an essential tool for data analysis, especially for big datasets
                        involving many predictors</li>
                    <li>Research into methods that give sparsity, such as the lassois an especially hot area</li>
                </ul>
            </section>


            <section>
                <iframe
                    src="https://embed.polleverywhere.com/multiple_choice_polls/7qQyqiMJB1dYvquCAd7s6?controls=none&short_poll=true"
                    width="1024px" height="768px"></iframe>
            </section>
        </div>
    </div>

    <script src="reveal.js/dist/reveal.js"></script>
    <script src="reveal.js/plugin/zoom/zoom.js"></script>
    <script src="reveal.js/plugin/notes/notes.js"></script>
    <script src="reveal.js/plugin/search/search.js"></script>
    <script src="reveal.js/plugin/markdown/markdown.js"></script>
    <script src="reveal.js/plugin/highlight/highlight.js"></script>
    <script src="reveal.js/plugin/math/math.js"></script>

    <script>
        // Also available as an ES module, see:
        // https://revealjs.com/initialization/
        Reveal.initialize({
            controls: false,
            progress: true,
            center: true,
            hash: true,
            transition: 'slide', // none/fade/slide/convex/concave/zoom

            width: 1024,
            height: 768,

            slideNumber: 'c/t',

            pdfSeparateFragments: false,

            math: {
                // mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                config: 'TeX-AMS_HTML-full',
                TeX: {
                    Macros: {
                        R: '\\mathbb{R}',
                        set: ['\\left\\{#1 \\; ; \\; #2\\right\\}', 2]
                    }
                }
            },

            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealMath, RevealHighlight]
        });

    </script>

</body>

</html>